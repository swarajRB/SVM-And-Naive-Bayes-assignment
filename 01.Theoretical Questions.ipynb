{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment - Support Vector Machine (SVM) & Naive Bayes**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **1. What is a Support Vector Machine (SVM)?**  \n",
    "\n",
    "- SVM is a **supervised learning algorithm** used for **classification and regression** tasks.  \n",
    "\n",
    "- It finds the **optimal hyperplane** that maximally separates different classes in a dataset.  \n",
    "\n",
    "- Works well for both **linear and non-linear** classification using the **kernel trick**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. What is the difference between Hard Margin and Soft Margin SVM?**  \n",
    "\n",
    "- **Hard Margin SVM**: Only works if the data is **linearly separable** (strict separation, no misclassification).  \n",
    "\n",
    "- **Soft Margin SVM**: Allows some misclassifications, controlled by **C parameter**, for handling noisy or overlapping data.  \n",
    "\n",
    "- Soft Margin is **more practical** for real-world datasets where perfect separation is rare.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3. What is the mathematical intuition behind SVM?**  \n",
    "- The objective is to **maximize the margin** (distance between the separating hyperplane and nearest points).  \n",
    "\n",
    "- The decision boundary is given by:  \n",
    "\n",
    "  w . x + b = 0\n",
    "   \n",
    "- The optimization problem minimizes **‖w‖²** while satisfying constraints for correctly classified points.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. What is the role of Lagrange Multipliers in SVM?**  \n",
    "\n",
    "- They are used in **constrained optimization** to solve the SVM objective function.  \n",
    "\n",
    "- Convert the problem into a **dual formulation**, making it easier to optimize with kernel functions.  \n",
    "\n",
    "- Ensure that only **support vectors contribute** to the final decision boundary.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. What are Support Vectors in SVM?**  \n",
    "\n",
    "- Data points that **lie closest** to the decision boundary (margin).  \n",
    "\n",
    "- These points **influence** the hyperplane's position and orientation.  \n",
    "\n",
    "- Removing a support vector **changes the boundary**, proving their importance.  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **6. What is a Support Vector Classifier (SVC)?**  \n",
    "\n",
    "- **SVC is the classification version** of SVM.  \n",
    "\n",
    "\n",
    "- Finds the **best hyperplane** to separate data into classes.  \n",
    "\n",
    "- Uses **kernels** to handle both **linear and non-linear** classification problems.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **7. What is a Support Vector Regressor (SVR)?**  \n",
    "\n",
    "- **SVR is the regression version** of SVM.  \n",
    "\n",
    "- It tries to fit a hyperplane such that most data points fall within a **margin (ε-tube)** around it.  \n",
    "\n",
    "- Controls error tolerance using the **epsilon (ε) parameter**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **8. What is the Kernel Trick in SVM?**  \n",
    "\n",
    "- A method to **transform non-linearly separable data** into a higher-dimensional space.  \n",
    "\n",
    "- Allows SVM to work with **complex decision boundaries**.  \n",
    "\n",
    "- Examples of kernels: **Linear, Polynomial, RBF, Sigmoid**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.**  \n",
    "\n",
    "- **Linear Kernel**: Best for **linearly separable** data, faster to compute.  \n",
    "\n",
    "- **Polynomial Kernel**: Captures **non-linear relationships** with degree \\(d\\).  \n",
    "\n",
    "- **RBF Kernel**: Most commonly used, handles **highly non-linear** data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10. What is the effect of the C parameter in SVM?**  \n",
    "\n",
    "- **Controls margin width**:  \n",
    "\n",
    "  - **High C** → Narrow margin, less misclassification (risk of overfitting).  \n",
    "  \n",
    "  - **Low C** → Wider margin, allows some misclassification (better generalization).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **11. What is the role of the Gamma parameter in RBF Kernel SVM?**  \n",
    "\n",
    "- Controls how **far a single training example’s influence reaches**.  \n",
    "\n",
    "- **High Gamma** → Each point has **high influence**, leading to overfitting.  \n",
    "\n",
    "- **Low Gamma** → Points influence a **larger region**, better generalization.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Naïve Bayes Classifier**  \n",
    "\n",
    "### **12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**  \n",
    "\n",
    "- A **probabilistic classifier** based on **Bayes’ Theorem**.  \n",
    "\n",
    "- Assumes that **features are independent**, which is often unrealistic (hence, \"Naïve\").  \n",
    "\n",
    "- Works well for **text classification, spam detection, sentiment analysis**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **13. What is Bayes’ Theorem?**  \n",
    "- A formula for updating probabilities based on new evidence: \n",
    "\n",
    "    P(A∣B)= {P(B∣A) . P(A)} / P(B)\n",
    "\n",
    "\n",
    "- Used in Naïve Bayes to **calculate class probabilities** given input features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.**  \n",
    "\n",
    "- **Gaussian Naïve Bayes**: Assumes data follows a **normal distribution** (used for continuous data).  \n",
    "\n",
    "- **Multinomial Naïve Bayes**: Used for **discrete count data** (e.g., text word counts).  \n",
    "\n",
    "- **Bernoulli Naïve Bayes**: Used for **binary feature data** (e.g., word presence/absence in text).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **15. When should you use Gaussian Naïve Bayes over other variants?**  \n",
    "\n",
    "- When features are **continuous and normally distributed**.  \n",
    "\n",
    "- Works well for **medical datasets, fraud detection, and sensor data**.  \n",
    "\n",
    "- If data is non-Gaussian, other models (like Decision Trees) may work better.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **16. What are the key assumptions made by Naïve Bayes?**  \n",
    "\n",
    "- **Feature independence** (each feature contributes independently to the outcome).  \n",
    "\n",
    "- **Equal importance of all features** (not always true in real-world data).  \n",
    "\n",
    "- **All features contribute to class probability estimation**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **17. What are the advantages and disadvantages of Naïve Bayes?**  \n",
    "\n",
    "**Advantages** :  \n",
    "\n",
    "- **Fast and efficient**, works well on **large datasets**.  \n",
    "\n",
    "- Performs well in **text classification** and spam filtering.  \n",
    "\n",
    "\n",
    "**Disadvantages** :  \n",
    "\n",
    "- Assumes **feature independence**, which is rarely true.  \n",
    "\n",
    "- Struggles with **correlated features** (e.g., height and weight).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **18. Why is Naïve Bayes a good choice for text classification?**  \n",
    "\n",
    "- **Handles high-dimensional data** well (text datasets have many features).  \n",
    "\n",
    "- **Fast training and prediction** (even on large datasets).  \n",
    "\n",
    "- **Robust to irrelevant features**, reducing noise in classification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **19. Compare SVM and Naïve Bayes for classification tasks.**  \n",
    "\n",
    "- **SVM** works well for **complex relationships** and **high-dimensional data**, while **Naïve Bayes** is best for **text classification** and **fast predictions**.  \n",
    "\n",
    "- **SVM is computationally expensive**, especially with kernels, whereas **Naïve Bayes is much faster** and works well with small datasets.  \n",
    "\n",
    "- **SVM handles non-linearity** using kernel tricks, but **Naïve Bayes assumes feature independence** and cannot model complex relationships.  \n",
    "\n",
    "- **SVM is sensitive to outliers**, while **Naïve Bayes is robust** since it relies on probability distributions.  \n",
    "\n",
    "- **Use SVM** for large, complex datasets and **Naïve Bayes** for fast, efficient classification, especially in NLP tasks.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **20. How does Laplace Smoothing help in Naïve Bayes?**  \n",
    "- Prevents **zero probability issues** when a word/feature is missing in training data.  \n",
    "- Adds a small constant **(α, usually 1)** to all counts to ensure no probability is exactly zero.  \n",
    "- Helps **generalize better**, avoiding overfitting in small datasets.  \n",
    "- Improves model performance, especially in **text classification** where many words may be unseen during training.  \n",
    "- Ensures **new/unseen words** still have a nonzero probability, making the model more robust.  \n",
    "\n",
    "   **Formula with Laplace Smoothing:**  \n",
    "\n",
    "      - P(w|c) = (count(w, c) +  ) / (count(c) + α x V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
